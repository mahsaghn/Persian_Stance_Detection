# -*- coding: utf-8 -*-
"""PSFeatureExtractor.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-zwppOfr0Cr1k15U5cOBA8Kd3xdKSXWr
"""

# -*- coding: utf-8 -*-
import nltk
import string

import pandas
import pandas as pd
import os, io, re
import numpy as np
import joblib
import os.path
import warnings
import stanza
import logging 

from hazm import *
from hazm import word_tokenize as hazm_word_tokenize
from sklearn.feature_extraction.text import TfidfVectorizer
from difflib import SequenceMatcher
from nltk import word_tokenize as nltk_word_tokenize
from openpyxl import load_workbook
from sklearn.feature_extraction.text import CountVectorizer
from config.baseline_h2c import FeatureExtractorConf
from transformers import AutoConfig, AutoTokenizer, TFAutoModel
"""BaseLineWithGridSearch.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cxXw92aQZKvWJGmOdKIYGGXRMpK_XNvQ

# Import Required Libraries
"""

warnings.filterwarnings('ignore')

#nltk.download('punkt')
#stanza.download('fa')
with open('dataset/refute_words.txt', 'r') as refute_file:
    refute_hedge_reporte_words = [w.replace('\n', '') for w in refute_file.readlines()]


class PSFeatureExtractor():

    def __init__(self, cfg: FeatureExtractorConf, load_data=False):
        self.cfg = cfg
        self.important_words = ['؟', 'تکذیب', 'تکذیب شد', ':']
        self.clean_claims_headlines = []
        self.clean_claims = []
        self.clean_headlines = []
        self.fa_stop_words = self.__get_stop_words()
        self.fa_punctuations = self.__get_ponctuations()
        self.denied_words = self.fa_stop_words + list(string.punctuation) + list(self.fa_punctuations)

        if load_data:
            self.claims, self.headlines, self.isQuestion, self.hasTowParts, self.labels = self.__read_dataset()
        else:
            self.claims, self.headlines, self.isQuestion, self.hasTowParts, self.labels = self.__generate_dataset()

    def __get_ponctuations(self):
        with open(self.cfg.ponctuations_path) as f:
            poncs = [i.replace('\n','') for i in f.readlines()]
        return poncs

    def __get_stop_words(self):
        normalizer = Normalizer()
        with open(self.cfg.stopWord_path) as f:
            lineList = [normalizer.normalize(line.rstrip("\n\r")) for line in f]
        return lineList

    def __get_polarity_ds(self):
        excel = load_workbook(filename=self.cfg.polarity_dataset_path)
        sheet = excel.active
        words_polarity_fa = {}
        for row in sheet.iter_rows(min_row=1):
            words_polarity_fa[row[0].value] = row[2].value
        return words_polarity_fa

    def __generate_dataset(self):
        data = pd.read_csv(self.cfg.dataset_path, encoding='utf-8')
        data = self.__remove_from_dataset(data)
        claims = np.array(data[self.cfg.claim_name].values)
        headlines = np.array(data[self.cfg.headline_name].values)
        labels = np.array(data[self.cfg.label_name].values)
        isQuestion = np.array([1 if '؟' in claim else 0 for claim in claims])
        hasTowParts = np.zeros(claims.shape[0])
        assert (claims.shape == headlines.shape == isQuestion.shape == labels.shape == hasTowParts.shape), "The features size are not equal."
        print('data shape is: ', claims.shape[0])
        return claims, headlines, isQuestion, hasTowParts, labels

    def __read_dataset(self):
        df = pd.read_csv(self.cfg.dataset_path, encoding='utf-8')
        df = self.__remove_from_dataset(df)
        claims = df[self.cfg.claim_name].values
        headlines = df[self.cfg.headline_name].values
        isQuestion = df[self.cfg.question_name].values
        hasTowParts = df[self.cfg.part_name].values
        labels = df[self.cfg.label_name].values
        assert (claims.shape == headlines.shape == isQuestion.shape == labels.shape == hasTowParts.shape), "The features size are not equal."
        print('data shape is: ',claims.shape)
        return claims, headlines, isQuestion, hasTowParts, labels

    def __remove_from_dataset(self, df):
        with open(self.cfg.uniq_claims_path) as f:
            uniq_claims = [i.replace('\n','') for i in f.readlines()]
        uni_number = 0
        if bool(uniq_claims):
            df['repeated'] = np.zeros(len(df), dtype=int)
            for claim_index in range(len(df[self.cfg.claim_name])):
                if (df[self.cfg.claim_name][claim_index] in uniq_claims) and uniq_claims[
                    df[self.cfg.claim_name][claim_index]] == df[self.cfg.headline_name][claim_index]:
                    df['repeated'][claim_index] = 1
                    print(df[self.cfg.claim_name][claim_index])
                    print(df[self.cfg.headline_name][claim_index])
                    uni_number += 1
            df.sort_values(by=['repeated'])
            print(df['repeated'])
        self.uniq_number = uni_number
        return df

    def __feature_vector_method(self, words, model, num_features):
        #index2word_set = set(model.wv.index2word)
        print(len(words),len(model[words[0]])) 
        featureVec = np.sum([model[word] for word in words if word in model],axis=0)# if word in index2word_set], axis=1)
        featureVec = np.divide(featureVec, len(words))
        return featureVec

    def clean_sentence(self, sentence):
        normalizer = Normalizer()
        shayee = normalizer.normalize("شایعه")
        patterns = ["(/(\s)*" + shayee + "(\s)*[0-9]+)|(/(\s)*شایعه(\s)*[0-9]+)",
                    "/(\s)*[0-9]+",
                    "\\u200c|\\u200d|\\u200e|\\u200b|\\u2067|\\u2069"]
        clean_sentences = sentence
        for pattern in patterns:
            x = re.search(pattern, clean_sentences)
            if x:
                clean_sentences = re.sub(pattern, "", clean_sentences)
        punc_regex = re.compile('|'.join(map(re.escape, list(string.punctuation) + list(self.fa_punctuations))))
        clean_sentences = punc_regex.sub("", clean_sentences)
        return clean_sentences

    def clean_tokens(self, target_list):
        assert isinstance(target_list, (list)) == True, "Type of target_list is not correct. It has to be list."
        normalizer = Normalizer()
        clean_words = [[i for i in item if normalizer.normalize(i) not in self.denied_words] for item in target_list]
        return clean_words

    def tokenize(self):
        if self.cfg.tokenize_method=='nltk':
            return self.nltk_tokenize()
        elif self.cfg.tokenize_method=='stanford':
            return self.stanford_tokenize()
        elif self.cfg.tokenize_method=='hazm':
            return self.hazm_tokenize()
        elif self.cfg.tokenize_method=='bert':
            return self.bert_tokenize()


    def bert_tokenize(self):
        parsbert_tokenizer = AutoTokenizer.from_pretrained(self.cfg.bert_model_path)
        self.tokens_claims = [parsbert_tokenizer.tokenize(claim) for claim in self.claims]
        self.tokens_claims = self.clean_tokens(target_list=[claim_result for claim_result in self.tokens_claims])
        self.tokens_headlines = [parsbert_tokenizer.tokenize(headline) for headline in self.headlines]
        self.tokens_headlines = self.clean_tokens(target_list=[headline_result for headline_result in self.tokens_headlines])
        for i in range(0, len(self.headlines)):
            clean_claim = parsbert_tokenizer.convert_tokens_to_string(self.tokens_claims[i])
            clean_headline = parsbert_tokenizer.convert_tokens_to_string(self.tokens_headlines[i])
            self.clean_claims_headlines.append(clean_claim + ' ' + clean_headline)
        print(self.clean_claims_headlines)
        return self.tokens_claims, self.tokens_headlines

    def stanford_tokenize(self, just_get_tokenized_words=False):
        nlp = stanza.Pipeline(lang='fa')
        claims_processors_result = []
        headlines_processors_result = []
        claims_tokenize = []
        headlines_tokenize = []

        for i in range(0, self.claims.shape[0]):
            clean_claim = self.clean_sentence(self.claims[i])
            doc = nlp(clean_claim)  # Run the pipeline on input text
            claims_processors_result.append(doc.sentences[0].words)
            claims_tokenize.append((obj.text for obj in doc.sentences[0].words))

            clean_headline = self.clean_sentence(self.headlines[i])
            doc = nlp(clean_headline)  # Run the pipeline on input text
            headlines_processors_result.append(doc.sentences[0].words)
            headlines_tokenize.append((obj.text for obj in doc.sentences[0].words))
            self.clean_claims_headlines.append(clean_claim + ' ' + clean_headline)

        self.tokens_claims = self.clean_tokens(target_list=claims_tokenize)
        self.tokens_headlines = self.clean_tokens(target_list=headlines_tokenize)
        if just_get_tokenized_words:
            return self.tokens_claims, self.tokens_headlines
        return claims_processors_result, headlines_processors_result

    def hazm_tokenize(self):
        claims_result = [self.clean_sentence(claim) for claim in self.claims]
        self.tokens_claims = self.clean_tokens(target_list=[hazm_word_tokenize(claim_result) for claim_result in claims_result])
        headlines_result = [self.clean_sentence(headline) for headline in self.headlines]
        self.tokens_headlines = self.clean_tokens(target_list=[hazm_word_tokenize(headline_result) for headline_result in headlines_result])
        self.clean_claims_headlines = [ ' '.join(self.tokens_claims[i] + self.tokens_headlines[i]) for i in range(0, self.claims.shape[0])]
        return self.tokens_claims, self.tokens_headlines

    def nltk_tokenize(self):
        claims_result = [self.clean_sentence(claim) for claim in self.claims]
        self.tokens_claims = self.clean_tokens(target_list=[nltk_word_tokenize(claim_result) for claim_result in claims_result])
        headlines_result = [self.clean_sentence(headline) for headline in self.headlines]
        self.tokens_headlines = self.clean_tokens( target_list=[nltk_word_tokenize(headline_result) for headline_result in headlines_result])
        self.clean_claims_headlines = [ ' '.join(self.tokens_claims[i] + self.tokens_headlines[i]) for i in range(0, self.claims.shape[0])]
        return self.tokens_claims, self.tokens_headlines

    def tf_idf(self):
        tfidf = TfidfVectorizer(sublinear_tf=True, min_df=10, norm='l2', ngram_range=(1, 2))
        features = tfidf.fit_transform(self.clean_claims_headlines).toarray()
        return features

    def w2v(self, model, num_features, target_sentences=None):
        if target_sentences is None:
            target_sentences = self.clean_claims_headlines
        reviewFeatureVecs = np.zeros((len(target_sentences), num_features), dtype="float32")
        for counter, sentence in enumerate(target_sentences):
            if counter % 500 == 0:
                print("data %d of %d" % (counter, len(target_sentences)))
            reviewFeatureVecs[counter] = self.__feature_vector_method(sentence, model, num_features)
        return reviewFeatureVecs

    def bow(self, target_sentences=None):
        if target_sentences is None:
            target_sentences = self.clean_claims_headlines
        vectorizer = CountVectorizer(ngram_range=(1, 2))
        X = vectorizer.fit_transform(target_sentences)
        return X.toarray()

    def similarity_fea(self):
        feature = []
        for i in range(0,len(self.claims)):
            claim = ' '.join(self.tokens_claims[i])
            headline = ' '.join(self.tokens_headlines[i])
            ratio = SequenceMatcher(None, claim, headline).ratio()
            quick_ratio = SequenceMatcher(None, claim, headline).quick_ratio()
            real_quick_ratio = SequenceMatcher(None, claim, headline).real_quick_ratio()
            feature.append([ratio, quick_ratio, real_quick_ratio])
        return feature

    def important_words_fea(self):
        assert (self.important_words != None), 'For calculating important words you should pass important words in initializer.'
        features = np.zeros((len(self.clean_claims_headlines), len(self.important_words)))
        for i in range(len(self.clean_claims_headlines)):
            for j in range(len(self.important_words)):
                if self.important_words[j] in self.clean_claims_headlines[i]:
                    features[i][j] = 1
        return features

    def root_distance_fea(self, target_sentences=None):  # target_sentences = clean_headlines
        if target_sentences == None:
            target_sentences = [' '.join(headline_tok) for headline_tok in self.tokens_headlines]
        nlp = stanza.Pipeline(lang='fa', models_dir=self.cfg.stanford_models_path, treebank=None, use_gpu=True)
        root_distance_feature = np.zeros((len(target_sentences), 1))
        for index, headline in enumerate(target_sentences):
            root_distance_feature[index] = -1
            doc = nlp(headline)
            root = [(i, doc.sentences[0].words[i].text) for i in range(len(doc.sentences[0].words)) if
                    doc.sentences[0].words[i].deprel == 'root']
            if (len(root) == 0):
                continue

            root_index, root_word = root[0]
            for word_index, word in enumerate(headline.split()):
                target = [(i, refute_hedge_reporte_words[i]) for i in range(len(refute_hedge_reporte_words)) if
                          refute_hedge_reporte_words[i] == word]
                if len(target) > 0:
                    target_index, target_word = target[0]
                    root_distance_feature[index] = abs(word_index - root_index)
                    break
        return root_distance_feature

    def polarity_fea(self, target_sentences=None):
        words_polarity_fa = self.__get_polarity_ds()

        if target_sentences == None:
            claims, headlines = self.tokens_claims, self.tokens_headlines
        else:
            mapped = list(target_sentences)
            claims, headlines = zip(*mapped)
        claims_array = np.asarray(claims)
        polarity_vector = np.zeros((len(claims_array), 30))
        for i, (claim, headline) in enumerate(zip(claims, headlines)):
            polars = []
            for j,word in enumerate(claim):
                if word in words_polarity_fa and words_polarity_fa[word]!=0.:
                    polars.append(words_polarity_fa[word])
                    if len(polars)==15:
                        break
            for j,word in enumerate(headline):
                if word in words_polarity_fa and words_polarity_fa[word]!=0.:
                    polars.append(words_polarity_fa[word])
                    if len(polars) == 30:
                        break
            if len(polars) < 30:
                polars = polars + [0.0]*(30-len(polars))
            polarity_vector[i] = polars
        return polarity_vector
    
    def load_vectors(self,fname):
        fin = io.open(fname, 'r', newline='\n', errors='ignore')
        n, d = map(int, fin.readline().split())
        data = {}
        for line in fin:
            tokens = line.rstrip().split(' ')
            data[tokens[0]] = map(float, tokens[1:])
        return data
    
    
    def generate_Features(self):
        features = self.isQuestion
        features = np.reshape(features, (len(features), 1))
        file_name = ''

        if self.cfg.load_if_exist or self.cfg.save_feature:
            if self.cfg.tfidf:
                file_name += 'tfidf_'
            if self.cfg.similarity:
                file_name += 'similarity_'
            if self.cfg.important_words:
                file_name += 'important_words_'
            if self.cfg.is_question:
                file_name += 'is_question_'
            if self.cfg.more_than2_parts:
                file_name += 'more_than2_parts_'
            if self.cfg.root_distance:
                file_name += 'root_distance_'
            if self.cfg.polarity:
                file_name += 'polarity_'
            if self.cfg.w2v:
                file_name += 'w2v_'
            if self.cfg.bow:
                file_name += 'bow_'

        if self.cfg.load_if_exist:
            assert len(self.cfg.load_path) > 0, "Please enter load_path."
            load_file_name = self.cfg.load_path + '/' + file_name + '.pkl'
            if os.path.isfile(load_file_name):
                features = joblib.load(load_file_name)
                logging.info('Features loaded successfully.')
                return features, file_name
            else:
                print('Features vector file is not exist.')
                # -------------- tfidf ----------
        if self.cfg.tfidf:
            print('Start to generate tf_idf feature')
            tf_idf_feature = self.tf_idf()
            features = np.append(features, tf_idf_feature, axis=1)
            logging.info('End of tf_idf feature')
        # -------------- similarity ----------
        if self.cfg.similarity:
            print('Start to generate similarity feature')
            similarity_feature = self.similarity_fea()
            features = np.append(features, similarity_feature, axis=1)
            logging.info('End of similarity feature')
        # -------------- important words ----------
        if self.cfg.important_words:
            print('Start to generate important words feature')
            important_words_feature = self.important_words_fea()
            features = np.append(features, important_words_feature, axis=1)
            logging.info('End of important words feature')
        # -------------- is question ----------
        if not self.cfg.is_question:
            features = features[:, 1:]
        else:
            logging.info('"is question" feature was added.')
        # -------------- more than tow parts ----------
        if self.cfg.more_than2_parts:
            features = np.append(features, np.reshape(self.hasTowParts, (len(self.hasTowParts), 1)), axis=1)
            logging.info('"more than tow parts" feature was added.')
        # -------------- root distance ----------
        if self.cfg.root_distance:
            print('Start to generate root distance feature')
            root_distance_feature = self.root_distance_fea()
            features = np.append(features, root_distance_feature, axis=1)
            logging.info('End of root distance feature')
        # -------------- root distance ----------
        if self.cfg.polarity:
            print('Start to generate polarity feature')
            polarity_feature = self.polarity_fea()
            features = np.append(features, polarity_feature, axis=1)
            logging.info('End of polarity feature')
            # -------------- w2v ----------
        if self.cfg.w2v:
            print('Start to generate w2v feature')
            assert len(self.cfg.w2v_model_path) > 0, "Please enter w2v_model_path."
            #w2v_model = joblib.load(self.cfg.w2v_model_path)
            w2v_model = {}
            fil = open(self.cfg.w2v_model_path, 'r', encoding='utf-8', errors='ignore')
            for line in fil:
                values = line.split()
                word = values[0].encode('utf-8').decode('utf-8')
                vector = np.asarray(values[1:], dtype='float32')
                w2v_model[word] = vector
            #w2v_model = fasttext.load_model(self.cfg.w2v_model_path)
            w2v_feature = self.w2v(w2v_model, num_features=300)
            w2v_feature = (w2v_feature - np.min(w2v_feature)) / (np.max(w2v_feature) - np.min(w2v_feature))
            features = np.append(features, w2v_feature, axis=1)
            logging.info('End of w2v feature')
            # -------------- bow ----------
        if self.cfg.bow:
            print('Start to generate bow feature')
            bow_feature = self.bow()
            features = np.append(features, bow_feature, axis=1)
            logging.info('End of bow feature')

        if self.cfg.save_feature:
            joblib.dump(features, (self.cfg.save_path + '/' + file_name + '.pkl'))
            logging.info('Features saved successfully.')
        return features, file_name
