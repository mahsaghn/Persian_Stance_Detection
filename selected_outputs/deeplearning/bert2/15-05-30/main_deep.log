[2021-09-24 15:05:30,706][root][INFO] - hazm tokenizer
[2021-09-24 15:05:33,994][filelock][INFO] - Lock 140524767602000 acquired on /root/.cache/huggingface/transformers/d3b7c3283a6a4ad4471f59269c9de8adadfab0b05eebf49a64e046fca56cdab2.58cfea678e7bd2c1de3bfd4a5357101526b9fbc32a994b9456047e55b0afbebe.lock
[2021-09-24 15:05:34,341][filelock][INFO] - Lock 140524767602000 released on /root/.cache/huggingface/transformers/d3b7c3283a6a4ad4471f59269c9de8adadfab0b05eebf49a64e046fca56cdab2.58cfea678e7bd2c1de3bfd4a5357101526b9fbc32a994b9456047e55b0afbebe.lock
[2021-09-24 15:05:35,056][filelock][INFO] - Lock 140524784159632 acquired on /root/.cache/huggingface/transformers/b80b05f64dc19f3c880b7074ef09108d0bc244e4b6f50d6dba094da0f1c231fd.6699f2ee4745b6531f79b9781879071b6ace2d2768df83889391421fb44d4474.lock
[2021-09-24 15:05:36,032][filelock][INFO] - Lock 140524784159632 released on /root/.cache/huggingface/transformers/b80b05f64dc19f3c880b7074ef09108d0bc244e4b6f50d6dba094da0f1c231fd.6699f2ee4745b6531f79b9781879071b6ace2d2768df83889391421fb44d4474.lock
[2021-09-24 15:05:44,245][filelock][INFO] - Lock 140524781118800 acquired on /root/.cache/huggingface/transformers/9cdad356c7d6956a9814a7cd822941b9a7bc7f3b450273d48f5a6626d97413a4.f8b29776d705d818b1b6d82629b12d40a9867a4c1e55e043dd81c99c2e36a246.h5.lock
[2021-09-24 15:06:07,593][filelock][INFO] - Lock 140524781118800 released on /root/.cache/huggingface/transformers/9cdad356c7d6956a9814a7cd822941b9a7bc7f3b450273d48f5a6626d97413a4.f8b29776d705d818b1b6d82629b12d40a9867a4c1e55e043dd81c99c2e36a246.h5.lock
[2021-09-24 15:06:11,618][tensorflow][WARNING] - The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
[2021-09-24 15:06:12,597][tensorflow][WARNING] - The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
[2021-09-24 15:06:15,552][root][INFO] - Model Summary:
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_ids (InputLayer)          [(None, 32)]         0                                            
__________________________________________________________________________________________________
attention_mask (InputLayer)     [(None, 32)]         0                                            
__________________________________________________________________________________________________
token_type_ids (InputLayer)     [(None, 32)]         0                                            
__________________________________________________________________________________________________
tf_bert_model (TFBertModel)     TFBaseModelOutputWit 162841344   input_ids[0][0]                  
                                                                 attention_mask[0][0]             
                                                                 token_type_ids[0][0]             
__________________________________________________________________________________________________
tf.math.reduce_mean (TFOpLambda (None, 768)          0           tf_bert_model[0][0]              
__________________________________________________________________________________________________
dense (Dense)                   (None, 32)           24608       tf.math.reduce_mean[0][0]        
__________________________________________________________________________________________________
dropout_37 (Dropout)            (None, 32)           0           dense[0][0]                      
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 16)           528         dropout_37[0][0]                 
__________________________________________________________________________________________________
classifier (Dense)              (None, 4)            68          dense_1[0][0]                    
==================================================================================================
Total params: 162,866,548
Trainable params: 162,866,548
Non-trainable params: 0
__________________________________________________________________________________________________
[2021-09-24 15:06:15,668][tensorflow][WARNING] - The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
[2021-09-24 15:06:15,739][tensorflow][WARNING] - The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
[2021-09-24 15:06:20,954][tensorflow][WARNING] - Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.
[2021-09-24 15:06:21,989][tensorflow][WARNING] - AutoGraph could not transform <function get_f1 at 0x7fcf4919d170> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: Unable to locate the source code of <function get_f1 at 0x7fcf4919d170>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
[2021-09-24 15:06:23,302][tensorflow][WARNING] - The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
[2021-09-24 15:06:23,354][tensorflow][WARNING] - The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
[2021-09-24 15:06:28,574][tensorflow][WARNING] - Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.
[2021-09-24 15:06:59,309][tensorflow][WARNING] - The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
[2021-09-24 15:06:59,364][tensorflow][WARNING] - The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
[2021-09-24 15:07:07,438][root][INFO] - Epoch: 0, loss: 1.336435, accuracy: 0.350361, precision: 0.482517, recall: 0.066506, get_f1: 0.113240, val_loss: 1.195278, val_accuracy: 0.443160, val_precision: 0.872340, val_recall: 0.078998, val_get_f1: 0.117046
[2021-09-24 15:07:36,952][root][INFO] - Epoch: 1, loss: 1.100561, accuracy: 0.526265, precision: 0.707031, recall: 0.261687, get_f1: 0.373564, val_loss: 0.919101, val_accuracy: 0.674374, val_precision: 0.803448, val_recall: 0.448940, val_get_f1: 0.504155
[2021-09-24 15:08:06,278][root][INFO] - Epoch: 2, loss: 0.952412, accuracy: 0.615904, precision: 0.769821, recall: 0.435181, get_f1: 0.561126, val_loss: 0.814994, val_accuracy: 0.705202, val_precision: 0.827160, val_recall: 0.516378, val_get_f1: 0.590508
[2021-09-24 15:08:35,597][root][INFO] - Epoch: 3, loss: 0.794743, accuracy: 0.698795, precision: 0.837124, recall: 0.549879, get_f1: 0.661322, val_loss: 0.682880, val_accuracy: 0.743738, val_precision: 0.823810, val_recall: 0.666667, val_get_f1: 0.710773
[2021-09-24 15:09:04,959][root][INFO] - Epoch: 4, loss: 0.661870, accuracy: 0.745060, precision: 0.870079, recall: 0.639036, get_f1: 0.739633, val_loss: 0.742263, val_accuracy: 0.761079, val_precision: 0.795652, val_recall: 0.705202, val_get_f1: 0.722645
[2021-09-24 15:09:31,963][root][INFO] - Epoch: 5, loss: 0.538323, accuracy: 0.798072, precision: 0.889284, recall: 0.723855, get_f1: 0.800489, val_loss: 0.734176, val_accuracy: 0.751445, val_precision: 0.810458, val_recall: 0.716763, val_get_f1: 0.733158
[2021-09-24 15:10:01,343][root][INFO] - Epoch: 6, loss: 0.446478, accuracy: 0.835181, precision: 0.918980, recall: 0.781687, get_f1: 0.846284, val_loss: 0.858931, val_accuracy: 0.784200, val_precision: 0.809816, val_recall: 0.763006, val_get_f1: 0.753325
[2021-09-24 15:10:28,353][root][INFO] - Epoch: 7, loss: 0.349746, accuracy: 0.860241, precision: 0.924078, recall: 0.821205, get_f1: 0.872139, val_loss: 1.005388, val_accuracy: 0.772640, val_precision: 0.795960, val_recall: 0.759152, val_get_f1: 0.746503
[2021-09-24 15:10:55,355][root][INFO] - Epoch: 8, loss: 0.301658, accuracy: 0.897831, precision: 0.944415, recall: 0.867952, get_f1: 0.906922, val_loss: 0.811250, val_accuracy: 0.780347, val_precision: 0.794715, val_recall: 0.753372, val_get_f1: 0.714321
[2021-09-24 15:11:22,360][root][INFO] - Epoch: 9, loss: 0.272475, accuracy: 0.919036, precision: 0.947614, recall: 0.880482, get_f1: 0.914156, val_loss: 1.015078, val_accuracy: 0.784200, val_precision: 0.801603, val_recall: 0.770713, val_get_f1: 0.799383
[2021-09-24 15:11:49,343][root][INFO] - Epoch: 10, loss: 0.217652, accuracy: 0.937349, precision: 0.960586, recall: 0.916145, get_f1: 0.940663, val_loss: 1.284272, val_accuracy: 0.759152, val_precision: 0.770459, val_recall: 0.743738, val_get_f1: 0.748725
[2021-09-24 15:12:16,332][root][INFO] - Epoch: 11, loss: 0.186045, accuracy: 0.939759, precision: 0.964629, recall: 0.920000, get_f1: 0.943544, val_loss: 1.259961, val_accuracy: 0.776493, val_precision: 0.785855, val_recall: 0.770713, val_get_f1: 0.747473
