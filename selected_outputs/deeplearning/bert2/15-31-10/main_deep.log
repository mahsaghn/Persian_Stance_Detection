[2021-09-24 15:31:10,983][root][INFO] - hazm tokenizer
[2021-09-24 15:31:27,096][tensorflow][WARNING] - The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
[2021-09-24 15:31:28,136][tensorflow][WARNING] - The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
[2021-09-24 15:31:31,209][root][INFO] - Model Summary:
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_ids (InputLayer)          [(None, 32)]         0                                            
__________________________________________________________________________________________________
attention_mask (InputLayer)     [(None, 32)]         0                                            
__________________________________________________________________________________________________
token_type_ids (InputLayer)     [(None, 32)]         0                                            
__________________________________________________________________________________________________
tf_bert_model (TFBertModel)     TFBaseModelOutputWit 162841344   input_ids[0][0]                  
                                                                 attention_mask[0][0]             
                                                                 token_type_ids[0][0]             
__________________________________________________________________________________________________
tf.math.reduce_mean (TFOpLambda (None, 768)          0           tf_bert_model[0][0]              
__________________________________________________________________________________________________
dense (Dense)                   (None, 32)           24608       tf.math.reduce_mean[0][0]        
__________________________________________________________________________________________________
dropout_37 (Dropout)            (None, 32)           0           dense[0][0]                      
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 16)           528         dropout_37[0][0]                 
__________________________________________________________________________________________________
classifier (Dense)              (None, 4)            68          dense_1[0][0]                    
==================================================================================================
Total params: 162,866,548
Trainable params: 162,866,548
Non-trainable params: 0
__________________________________________________________________________________________________
[2021-09-24 15:31:31,328][tensorflow][WARNING] - The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
[2021-09-24 15:31:31,385][tensorflow][WARNING] - The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
[2021-09-24 15:31:36,759][tensorflow][WARNING] - Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.
[2021-09-24 15:31:37,798][tensorflow][WARNING] - AutoGraph could not transform <function get_f1 at 0x7fc31bb76170> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: Unable to locate the source code of <function get_f1 at 0x7fc31bb76170>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
[2021-09-24 15:31:39,114][tensorflow][WARNING] - The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
[2021-09-24 15:31:39,167][tensorflow][WARNING] - The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
[2021-09-24 15:31:44,502][tensorflow][WARNING] - Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.
[2021-09-24 15:32:15,196][tensorflow][WARNING] - The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
[2021-09-24 15:32:15,252][tensorflow][WARNING] - The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
[2021-09-24 15:32:23,232][root][INFO] - Epoch: 0, loss: 1.279561, accuracy: 0.410602, precision: 0.536269, recall: 0.099759, get_f1: 0.174548, val_loss: 1.144230, val_accuracy: 0.533719, val_precision: 0.608434, val_recall: 0.194605, val_get_f1: 0.276860
[2021-09-24 15:32:52,805][root][INFO] - Epoch: 1, loss: 1.021749, accuracy: 0.581205, precision: 0.720696, recall: 0.379277, get_f1: 0.491844, val_loss: 0.868078, val_accuracy: 0.653179, val_precision: 0.843260, val_recall: 0.518304, val_get_f1: 0.518518
[2021-09-24 15:33:22,582][root][INFO] - Epoch: 2, loss: 0.836905, accuracy: 0.679518, precision: 0.803779, recall: 0.533012, get_f1: 0.641663, val_loss: 0.748337, val_accuracy: 0.712909, val_precision: 0.857527, val_recall: 0.614644, val_get_f1: 0.621102
[2021-09-24 15:33:49,672][root][INFO] - Epoch: 3, loss: 0.708051, accuracy: 0.738795, precision: 0.851687, recall: 0.644819, get_f1: 0.732602, val_loss: 0.718235, val_accuracy: 0.705202, val_precision: 0.796163, val_recall: 0.639692, val_get_f1: 0.650008
[2021-09-24 15:34:18,998][root][INFO] - Epoch: 4, loss: 0.580367, accuracy: 0.777831, precision: 0.885373, recall: 0.714699, get_f1: 0.789383, val_loss: 0.697340, val_accuracy: 0.734104, val_precision: 0.827830, val_recall: 0.676301, val_get_f1: 0.678379
[2021-09-24 15:34:48,244][root][INFO] - Epoch: 5, loss: 0.515244, accuracy: 0.815904, precision: 0.903282, recall: 0.756145, get_f1: 0.820078, val_loss: 0.786033, val_accuracy: 0.757225, val_precision: 0.795402, val_recall: 0.666667, val_get_f1: 0.656377
[2021-09-24 15:35:17,715][root][INFO] - Epoch: 6, loss: 0.421177, accuracy: 0.855904, precision: 0.929714, recall: 0.784096, get_f1: 0.849075, val_loss: 0.708401, val_accuracy: 0.782274, val_precision: 0.817330, val_recall: 0.672447, val_get_f1: 0.701318
[2021-09-24 15:35:44,721][root][INFO] - Epoch: 7, loss: 0.334654, accuracy: 0.881928, precision: 0.941335, recall: 0.842892, get_f1: 0.888833, val_loss: 0.796337, val_accuracy: 0.772640, val_precision: 0.812632, val_recall: 0.743738, val_get_f1: 0.755558
[2021-09-24 15:36:11,726][root][INFO] - Epoch: 8, loss: 0.267722, accuracy: 0.908434, precision: 0.936280, recall: 0.878072, get_f1: 0.906077, val_loss: 0.942700, val_accuracy: 0.772640, val_precision: 0.799180, val_recall: 0.751445, val_get_f1: 0.763276
[2021-09-24 15:36:41,091][root][INFO] - Epoch: 9, loss: 0.270234, accuracy: 0.901205, precision: 0.937146, recall: 0.876626, get_f1: 0.905703, val_loss: 0.837047, val_accuracy: 0.789981, val_precision: 0.806061, val_recall: 0.768786, val_get_f1: 0.783359
[2021-09-24 15:37:08,085][root][INFO] - Epoch: 10, loss: 0.209293, accuracy: 0.931566, precision: 0.951330, recall: 0.913735, get_f1: 0.932475, val_loss: 0.958028, val_accuracy: 0.755299, val_precision: 0.778004, val_recall: 0.736031, val_get_f1: 0.775317
[2021-09-24 15:37:37,481][root][INFO] - Epoch: 11, loss: 0.182019, accuracy: 0.939759, precision: 0.956175, recall: 0.925301, get_f1: 0.940522, val_loss: 0.880739, val_accuracy: 0.791907, val_precision: 0.810865, val_recall: 0.776493, val_get_f1: 0.778113
[2021-09-24 15:38:04,451][root][INFO] - Epoch: 12, loss: 0.143000, accuracy: 0.950361, precision: 0.963511, recall: 0.941687, get_f1: 0.954642, val_loss: 1.086185, val_accuracy: 0.780347, val_precision: 0.789683, val_recall: 0.766859, val_get_f1: 0.765955
[2021-09-24 15:38:31,464][root][INFO] - Epoch: 13, loss: 0.107702, accuracy: 0.963373, precision: 0.977800, recall: 0.955181, get_f1: 0.966070, val_loss: 1.236559, val_accuracy: 0.778420, val_precision: 0.789784, val_recall: 0.774566, val_get_f1: 0.769335
[2021-09-24 15:38:58,477][root][INFO] - Epoch: 14, loss: 0.104373, accuracy: 0.960482, precision: 0.970603, recall: 0.954699, get_f1: 0.957870, val_loss: 1.199368, val_accuracy: 0.791907, val_precision: 0.803181, val_recall: 0.778420, val_get_f1: 0.776300
[2021-09-24 15:39:25,510][root][INFO] - Epoch: 15, loss: 0.085820, accuracy: 0.974940, precision: 0.982353, recall: 0.965783, get_f1: 0.974306, val_loss: 1.356642, val_accuracy: 0.755299, val_precision: 0.763209, val_recall: 0.751445, val_get_f1: 0.749152
[2021-09-24 15:39:52,493][root][INFO] - Epoch: 16, loss: 0.087259, accuracy: 0.970120, precision: 0.978059, recall: 0.966747, get_f1: 0.973588, val_loss: 1.372732, val_accuracy: 0.763006, val_precision: 0.765504, val_recall: 0.761079, val_get_f1: 0.754001
