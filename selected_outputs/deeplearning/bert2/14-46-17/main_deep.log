[2021-09-24 14:46:17,968][root][INFO] - hazm tokenizer
[2021-09-24 14:46:33,556][tensorflow][WARNING] - The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
[2021-09-24 14:46:34,564][tensorflow][WARNING] - The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
[2021-09-24 14:46:37,552][root][INFO] - Model Summary:
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_ids (InputLayer)          [(None, 32)]         0                                            
__________________________________________________________________________________________________
attention_mask (InputLayer)     [(None, 32)]         0                                            
__________________________________________________________________________________________________
token_type_ids (InputLayer)     [(None, 32)]         0                                            
__________________________________________________________________________________________________
tf_bert_model (TFBertModel)     TFBaseModelOutputWit 118297344   input_ids[0][0]                  
                                                                 attention_mask[0][0]             
                                                                 token_type_ids[0][0]             
__________________________________________________________________________________________________
tf.math.reduce_mean (TFOpLambda (None, 768)          0           tf_bert_model[0][0]              
__________________________________________________________________________________________________
dense (Dense)                   (None, 32)           24608       tf.math.reduce_mean[0][0]        
__________________________________________________________________________________________________
dropout_37 (Dropout)            (None, 32)           0           dense[0][0]                      
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 16)           528         dropout_37[0][0]                 
__________________________________________________________________________________________________
classifier (Dense)              (None, 4)            68          dense_1[0][0]                    
==================================================================================================
Total params: 118,322,548
Trainable params: 118,322,548
Non-trainable params: 0
__________________________________________________________________________________________________
[2021-09-24 14:46:37,679][tensorflow][WARNING] - The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
[2021-09-24 14:46:37,733][tensorflow][WARNING] - The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
[2021-09-24 14:46:42,874][tensorflow][WARNING] - Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.
[2021-09-24 14:46:43,921][tensorflow][WARNING] - AutoGraph could not transform <function get_f1 at 0x7f10d368f200> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: Unable to locate the source code of <function get_f1 at 0x7f10d368f200>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
[2021-09-24 14:46:45,275][tensorflow][WARNING] - The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
[2021-09-24 14:46:45,327][tensorflow][WARNING] - The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
[2021-09-24 14:46:50,509][tensorflow][WARNING] - Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.
[2021-09-24 14:47:20,695][tensorflow][WARNING] - The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
[2021-09-24 14:47:20,748][tensorflow][WARNING] - The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
[2021-09-24 14:47:27,635][root][INFO] - Epoch: 0, loss: 1.338171, accuracy: 0.328193, precision: 0.316901, recall: 0.021687, get_f1: 0.040480, val_loss: 1.306709, val_accuracy: 0.350674, val_precision: 0.466019, val_recall: 0.092486, val_get_f1: 0.124777
[2021-09-24 14:47:55,698][root][INFO] - Epoch: 1, loss: 1.288026, accuracy: 0.366265, precision: 0.452830, recall: 0.057831, get_f1: 0.104330, val_loss: 1.239139, val_accuracy: 0.429672, val_precision: 0.764706, val_recall: 0.025048, val_get_f1: 0.039016
[2021-09-24 14:48:23,743][root][INFO] - Epoch: 2, loss: 1.266906, accuracy: 0.422169, precision: 0.570850, recall: 0.135904, get_f1: 0.224119, val_loss: 1.119203, val_accuracy: 0.562620, val_precision: 0.700000, val_recall: 0.242775, val_get_f1: 0.290840
[2021-09-24 14:48:52,450][root][INFO] - Epoch: 3, loss: 1.110034, accuracy: 0.531566, precision: 0.700739, recall: 0.274217, get_f1: 0.391535, val_loss: 1.013868, val_accuracy: 0.587669, val_precision: 0.789256, val_recall: 0.368015, val_get_f1: 0.404755
[2021-09-24 14:49:20,597][root][INFO] - Epoch: 4, loss: 1.000015, accuracy: 0.585542, precision: 0.753425, recall: 0.371084, get_f1: 0.489810, val_loss: 0.963415, val_accuracy: 0.637765, val_precision: 0.838583, val_recall: 0.410405, val_get_f1: 0.444849
[2021-09-24 14:49:48,588][root][INFO] - Epoch: 5, loss: 0.885221, accuracy: 0.652048, precision: 0.815141, recall: 0.446265, get_f1: 0.574760, val_loss: 0.931565, val_accuracy: 0.682081, val_precision: 0.833333, val_recall: 0.452794, val_get_f1: 0.473021
[2021-09-24 14:50:15,059][root][INFO] - Epoch: 6, loss: 0.743621, accuracy: 0.721446, precision: 0.858333, recall: 0.546024, get_f1: 0.666130, val_loss: 0.940459, val_accuracy: 0.658960, val_precision: 0.702638, val_recall: 0.564547, val_get_f1: 0.541683
[2021-09-24 14:50:43,179][root][INFO] - Epoch: 7, loss: 0.639219, accuracy: 0.743614, precision: 0.866578, recall: 0.626024, get_f1: 0.726665, val_loss: 0.931213, val_accuracy: 0.710983, val_precision: 0.769036, val_recall: 0.583815, val_get_f1: 0.606987
[2021-09-24 14:51:09,681][root][INFO] - Epoch: 8, loss: 0.578376, accuracy: 0.785060, precision: 0.890798, recall: 0.699759, get_f1: 0.778448, val_loss: 1.047265, val_accuracy: 0.685934, val_precision: 0.724299, val_recall: 0.597302, val_get_f1: 0.593734
[2021-09-24 14:51:36,192][root][INFO] - Epoch: 9, loss: 0.450193, accuracy: 0.829398, precision: 0.910380, recall: 0.773494, get_f1: 0.840203, val_loss: 1.144799, val_accuracy: 0.672447, val_precision: 0.713656, val_recall: 0.624277, val_get_f1: 0.598195
[2021-09-24 14:52:02,675][root][INFO] - Epoch: 10, loss: 0.365482, accuracy: 0.866024, precision: 0.929304, recall: 0.829880, get_f1: 0.873459, val_loss: 1.276463, val_accuracy: 0.697495, val_precision: 0.717573, val_recall: 0.660886, val_get_f1: 0.639011
[2021-09-24 14:52:29,175][root][INFO] - Epoch: 11, loss: 0.288780, accuracy: 0.887229, precision: 0.942166, recall: 0.863614, get_f1: 0.898754, val_loss: 1.497075, val_accuracy: 0.655106, val_precision: 0.674134, val_recall: 0.637765, val_get_f1: 0.626208
[2021-09-24 14:52:55,731][root][INFO] - Epoch: 12, loss: 0.342963, accuracy: 0.872771, precision: 0.928080, recall: 0.845783, get_f1: 0.887292, val_loss: 1.463707, val_accuracy: 0.705202, val_precision: 0.730435, val_recall: 0.647399, val_get_f1: 0.651213
