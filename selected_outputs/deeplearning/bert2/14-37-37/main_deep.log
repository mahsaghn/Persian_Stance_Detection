[2021-09-24 14:37:38,130][root][INFO] - hazm tokenizer
[2021-09-24 14:37:42,971][filelock][INFO] - Lock 140264631855568 acquired on /root/.cache/huggingface/transformers/69e6fc93aba72adbdbe587dd870aa6e40074b2852ae7f827430f79554d1b474d.75ba9b0b397e5ff811df7979fa501544a6dfde55eb643e16f670d7aa16f81c1d.lock
[2021-09-24 14:37:43,728][filelock][INFO] - Lock 140264631855568 released on /root/.cache/huggingface/transformers/69e6fc93aba72adbdbe587dd870aa6e40074b2852ae7f827430f79554d1b474d.75ba9b0b397e5ff811df7979fa501544a6dfde55eb643e16f670d7aa16f81c1d.lock
[2021-09-24 14:37:44,091][filelock][INFO] - Lock 140264631884304 acquired on /root/.cache/huggingface/transformers/27cf38fbd3f491fa577a35ed855e45e2f176b4c39a8377b4845e1c91550b31d7.473194e90cfe872b63370ba003df2403b89fffe014fa356ea877b47f35f125fe.lock
[2021-09-24 14:37:44,935][filelock][INFO] - Lock 140264631884304 released on /root/.cache/huggingface/transformers/27cf38fbd3f491fa577a35ed855e45e2f176b4c39a8377b4845e1c91550b31d7.473194e90cfe872b63370ba003df2403b89fffe014fa356ea877b47f35f125fe.lock
[2021-09-24 14:37:45,623][filelock][INFO] - Lock 140264631903632 acquired on /root/.cache/huggingface/transformers/66ef075eb41504b8392a7755677a74bc54bdc0400035e67cc5f55c0af9a1b0a7.f982506b52498d4adb4bd491f593dc92b2ef6be61bfdbe9d30f53f963f9f5b66.lock
[2021-09-24 14:37:45,966][filelock][INFO] - Lock 140264631903632 released on /root/.cache/huggingface/transformers/66ef075eb41504b8392a7755677a74bc54bdc0400035e67cc5f55c0af9a1b0a7.f982506b52498d4adb4bd491f593dc92b2ef6be61bfdbe9d30f53f963f9f5b66.lock
[2021-09-24 14:37:51,920][filelock][INFO] - Lock 140264630133776 acquired on /root/.cache/huggingface/transformers/d979524b1656b37df949205579e294a8630d146ed38c49fe58ce7bb917a2b1c2.87857ef6404df792165ed17961775e38d27a18be72a85d062129517ff95781b1.h5.lock
[2021-09-24 14:38:06,906][filelock][INFO] - Lock 140264630133776 released on /root/.cache/huggingface/transformers/d979524b1656b37df949205579e294a8630d146ed38c49fe58ce7bb917a2b1c2.87857ef6404df792165ed17961775e38d27a18be72a85d062129517ff95781b1.h5.lock
[2021-09-24 14:38:12,756][tensorflow][WARNING] - The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
[2021-09-24 14:38:13,770][tensorflow][WARNING] - The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
[2021-09-24 14:38:16,828][root][INFO] - Model Summary:
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_ids (InputLayer)          [(None, 32)]         0                                            
__________________________________________________________________________________________________
attention_mask (InputLayer)     [(None, 32)]         0                                            
__________________________________________________________________________________________________
token_type_ids (InputLayer)     [(None, 32)]         0                                            
__________________________________________________________________________________________________
tf_bert_model (TFBertModel)     TFBaseModelOutputWit 118297344   input_ids[0][0]                  
                                                                 attention_mask[0][0]             
                                                                 token_type_ids[0][0]             
__________________________________________________________________________________________________
tf.math.reduce_mean (TFOpLambda (None, 768)          0           tf_bert_model[0][0]              
__________________________________________________________________________________________________
dense (Dense)                   (None, 32)           24608       tf.math.reduce_mean[0][0]        
__________________________________________________________________________________________________
dropout_37 (Dropout)            (None, 32)           0           dense[0][0]                      
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 16)           528         dropout_37[0][0]                 
__________________________________________________________________________________________________
classifier (Dense)              (None, 4)            68          dense_1[0][0]                    
==================================================================================================
Total params: 118,322,548
Trainable params: 118,322,548
Non-trainable params: 0
__________________________________________________________________________________________________
[2021-09-24 14:38:17,030][tensorflow][WARNING] - The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
[2021-09-24 14:38:17,084][tensorflow][WARNING] - The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
[2021-09-24 14:38:22,356][tensorflow][WARNING] - Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.
[2021-09-24 14:38:23,382][tensorflow][WARNING] - AutoGraph could not transform <function get_f1 at 0x7f92b6cfc200> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: Unable to locate the source code of <function get_f1 at 0x7f92b6cfc200>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
[2021-09-24 14:38:24,755][tensorflow][WARNING] - The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
[2021-09-24 14:38:24,814][tensorflow][WARNING] - The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
[2021-09-24 14:38:29,952][tensorflow][WARNING] - Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.
[2021-09-24 14:39:00,566][tensorflow][WARNING] - The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
[2021-09-24 14:39:00,619][tensorflow][WARNING] - The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
